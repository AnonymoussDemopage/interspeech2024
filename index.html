<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



  <title>Vec-Tok-VC+</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="TODO: title">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="summary">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
  <style>
    .method {
      display: inline-block;
      /*             width: 120px; /* Adjust the width as needed */
      */ font-weight: bold;
    }

    .explanation {
      display: inline-block;
      /*             margin-left: 20px; /* Adjust the margin as needed */
      */
    }
  </style>
</head>





<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
        <!-- <h1 class="project-name">Demo PAGE</h1> -->
        <!-- <h2 class="project-tagline"></h2> -->
    </section>

    <section class="main-content">
        <h1 id="">
            <center>Vec-Tok-VC+: Residual-enhanced Robust Zero-shot Voice Conversion with Progressive Constraints in a Dual-mode Training Strategy</center>
        </h1>

        <h3 id="">
            <center>Anonymous submission to Interspeech 2024</center>
        </h3>

        <br><br>
        <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
        <p>Zero-shot voice conversion (VC) aims to transform source speech into arbitrary unseen target voice while keeping the linguistic content unchanged. 
            Recent VC methods have made significant progress, but semantic losses in the decoupling process as well as training-inference mismatch still 
            hinder conversion performance. In this paper, we propose Vec-Tok-VC+, a novel prompt-based zero-shot VC model improved from Vec-Tok Codec, 
            achieving voice conversion given only a 3s target speaker prompt. We design a residual-enhanced K-Means decoupler to enhance the semantic content
            extraction with a two-layer clustering process. Besides, we employ teacherguided refinement to simulate the conversion process to eliminate the 
            training-inference mismatch, forming a dual-mode training strategy. Furthermore, we design a multi-codebook progressive loss function to constrain
            the layer-wise output of the model from coarse to fine to improve speaker similarity and content accuracy. Objective and subjective evaluations 
            demonstrate that Vec-Tok-VC+ outperforms the strong baselines in naturalness, intelligibility, and speaker similarity.</p>
        <br><br>

        <table border=0 frame=void rules=none>
            <tr>
              <center><img src='fig/overview.png' width="60%"></center>
              <center><span><b>Figure 1. Overview of Vec-Tok-VC+</b></span></center>
            </tr>
            <tr><br></tr>
            <tr>
              <td>
                <center><img src='fig/components.png' width="80%"></center>
                <center><span><b>Figure 2. The details of Vec-Tok-VC+. (a): the residual-enhanced K-Means decoupler. (b): the dual-mode teacher guidance module. (c): the converter and multi-codebook progressive constraint.</b></span> </center>
              </td>
            </tr>

          </table>
          <br><br>














        <footer class="site-footer">
            <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHubPages</a>.</span>
        </footer>
    </section>
</body>

</html>
